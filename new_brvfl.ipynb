{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pymc3 as pm\n",
    "\n",
    "class Model:\n",
    "    def one_hot_encoding(self, label, n_class):\n",
    "        y = np.zeros([len(label), n_class])\n",
    "        for i in range(len(label)):\n",
    "            y[i, label[i]] = 1\n",
    "        return y\n",
    "    \n",
    "    def standardize(self, x):\n",
    "        if self.same_feature is True:\n",
    "            if self.data_std is None:\n",
    "                self.data_std = np.maximum(np.std(x), 1/np.sqrt(len(x)))\n",
    "            if self.data_mean is None:\n",
    "                self.data_mean = np.mean(x)\n",
    "            return (x - self.data_mean) / self.data_std\n",
    "        else:\n",
    "            if self.data_std is None:\n",
    "                self.data_std = np.maximum(np.std(x, axis=0), 1/np.sqrt(len(x)))\n",
    "            if self.data_mean is None:\n",
    "                self.data_mean = np.mean(x, axis=0)\n",
    "            return (x - self.data_mean) / self.data_std\n",
    "        \n",
    "    def softmax(self, x):\n",
    "        return np.exp(x) / np.repeat((np.sum(np.exp(x), axis=1))[:, np.newaxis], len(x[0]), axis=1)\n",
    "\n",
    "class Activation:\n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.e ** (-x))\n",
    "    \n",
    "    def sine(self, x):\n",
    "        return np.sin(x)\n",
    "    \n",
    "    def sign(self, x):\n",
    "        return np.sign(x)\n",
    "    \n",
    "    def relu(self, x):\n",
    "        return np.maximum(0, x)\n",
    "\n",
    "class BRVFL(Model):\n",
    "    \"\"\" BRVFL Classifier \"\"\"\n",
    "\n",
    "    def __init__(self, n_node, w_range, b_range, n_layer=1, alpha_1=10**(-5), alpha_2=10**(-5), alpha_3=10**(-5), alpha_4=10**(-5), n_iter=1000, tol=1.0e-3, activation='sigmoid', same_feature=False):\n",
    "        self.n_node = n_node\n",
    "        self.w_range = w_range\n",
    "        self.b_range = b_range\n",
    "        self.n_layer = n_layer\n",
    "        \n",
    "        self.alpha_1 = alpha_1 # Gamma distribution parameter\n",
    "        self.alpha_2 = alpha_2\n",
    "        self.alpha_3 = alpha_3\n",
    "        self.alpha_4 = alpha_4\n",
    "        self.n_iter = n_iter\n",
    "        self.tol = tol\n",
    "\n",
    "        self.weight = None\n",
    "        self.bias = None\n",
    "        self.beta = None\n",
    "        self.prec = None\n",
    "        self.var = None\n",
    "        a = Activation()\n",
    "        self.activation_function = getattr(a, activation)\n",
    "        self.data_std = None\n",
    "        self.data_mean = None\n",
    "        self.same_feature = same_feature\n",
    "\n",
    "    def train(self, data, label, n_class):\n",
    "        assert len(data.shape) > 1\n",
    "        assert len(data) == len(label)\n",
    "        assert len(label.shape) == 1\n",
    "\n",
    "        data = self.standardize(data)\n",
    "        n_sample, n_feature = np.shape(data)\n",
    "        self.weight = (self.w_range[1] - self.w_range[0]) * np.random.random([n_feature, self.n_node]) + self.w_range[0]\n",
    "        self.bias = (self.b_range[1] - self.b_range[0]) * np.random.random([1, self.n_node]) + self.b_range[0]\n",
    "        \n",
    "        h = self.activation_function(np.dot(data, self.weight) + np.dot(np.ones([n_sample, 1]), self.bias))\n",
    "        d = np.concatenate([h, data], axis=1)\n",
    "        d = np.concatenate([d, np.ones_like(d[:, 0:1])], axis=1)\n",
    "        y = self.one_hot_encoding(label, n_class)\n",
    "        dT_y = np.dot(d.T, y)\n",
    "        dT_d = np.dot(d.T, d)\n",
    "        eigen_val = np.linalg.eigvalsh(dT_d)\n",
    "\n",
    "        # Initialize variance and precision using Evidence approximation\n",
    "        model = pm.Model()\n",
    "        with model:\n",
    "            p = pm.Gamma('p', alpha=self.alpha_1, beta=self.alpha_2)\n",
    "            v = pm.Gamma('v', alpha=self.alpha_3, beta=self.alpha_4)\n",
    "            b = pm.Normal('b', mu=0, tau=p, shape=(len(d[0]), n_class))\n",
    "            y_obs = pm.Normal('y_obs', mu=pm.math.dot(d, b), tau=v, observed=y)\n",
    "        \n",
    "        map_estimate =  pm.find_MAP(model=model, progressbar=False)\n",
    "        self.prec, self.var, self.beta = map_estimate['p'].item(0), map_estimate['v'].item(0), map_estimate['b']\n",
    "\n",
    "        # Iterate to meet convergence criteria\n",
    "        mean_prev = None\n",
    "        for iter_ in range(self.n_iter):\n",
    "            # Posterior update\n",
    "            # update posterior covariance\n",
    "            covar = np.linalg.inv(self.prec * np.identity(dT_d.shape[1]) + dT_d / self.var)\n",
    "            # update posterior mean\n",
    "            mean = np.dot(covar, dT_y) / self.var\n",
    "\n",
    "            # Hyperparameters update\n",
    "            # update eigenvalues\n",
    "            lam = eigen_val / self.var\n",
    "            # update precision and variance \n",
    "            delta = np.sum(np.divide(lam, lam + self.prec))\n",
    "            self.prec = (delta + 2 * self.alpha_1) / (np.sum(np.square(mean)) + 2 * self.alpha_2)\n",
    "            self.var = (np.sum(np.square(y - np.dot(d, self.beta))) + self.alpha_4) / (n_sample + delta + 2 * self.alpha_3)\n",
    "\n",
    "            # Check for convergence\n",
    "            if iter_ != 0 and np.sum(np.abs(mean_prev - mean)) < self.tol:\n",
    "                print(\" Convergence after \", str(iter_), \" iterations\", end='')\n",
    "                break\n",
    "            mean_prev = np.copy(mean)\n",
    "\n",
    "        # Final Posterior update\n",
    "        # update posterior covariance\n",
    "        covar = np.linalg.inv(self.prec * np.identity(dT_d.shape[1]) + dT_d / self.var)\n",
    "        # update posterior mean\n",
    "        self.beta = np.dot(covar, dT_y) / self.var\n",
    "\n",
    "    def predict(self, data, raw_output=False):\n",
    "        data = self.standardize(data) # Normalize\n",
    "        h = self.activation_function(np.dot(data, self.weight) + self.bias)\n",
    "        d = np.concatenate([h, data], axis=1)\n",
    "        d = np.concatenate([d, np.ones_like(d[:, 0:1])], axis=1)\n",
    "        result = self.softmax(np.dot(d, self.beta))\n",
    "        if not raw_output:\n",
    "            result = np.argmax(result, axis=1)\n",
    "        return result\n",
    "\n",
    "    def eval(self, data, label):\n",
    "        assert len(data.shape) > 1\n",
    "        assert len(data) == len(label)\n",
    "        assert len(label.shape) == 1\n",
    "        \n",
    "        result = self.predict(data, False)\n",
    "        acc = np.sum(np.equal(result, label))/len(label)\n",
    "        return acc\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
