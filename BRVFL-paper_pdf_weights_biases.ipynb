{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ffc05559",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import sklearn.datasets as sk_dataset\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "from scipy.io import loadmat\n",
    "\n",
    "import pymc3 as pm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77fdbbb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize(x):\n",
    "    std = np.maximum(np.std(x, axis=0), 1/np.sqrt(len(x)))\n",
    "    mean = np.mean(x, axis=0)\n",
    "    return (x - mean) / std\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def one_hot_encoding(label, n_class):\n",
    "    y = np.zeros([len(label), n_class])\n",
    "    for i in range(len(label)):\n",
    "        y[i, label[i]] = 1\n",
    "    return y\n",
    "\n",
    "def softmax(x):\n",
    "    return np.exp(x) / np.repeat((np.sum(np.exp(x), axis=1))[:, np.newaxis], len(x[0]), axis=1)\n",
    "\n",
    "def sigmoid_kernel(X, beta, weights, bias):\n",
    "    n_sample = len(X)\n",
    "    h = relu(np.dot(X, weights) + np.dot(np.ones([n_sample, 1]), bias))\n",
    "    d = np.concatenate([h, X], axis=1)\n",
    "    out = np.dot(d, beta)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ab78148",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_node = 10\n",
    "n_iter = 1000\n",
    "lam = 1 # regularization parameter, lambda\n",
    "w_range = [-1, 1] # range of random weights\n",
    "b_range = [0, 1] # range of random biases\n",
    "alpha_1 = 10**(-5) # Gamma distribution parameter\n",
    "alpha_2 = 10**(-5)\n",
    "alpha_3 = 10**(-5)\n",
    "alpha_4 = 10**(-5)\n",
    "tol = 1.0e-3\n",
    "\n",
    "dataset = loadmat('coil20.mat')\n",
    "label = np.array([dataset['Y'][i][0] - 1 for i in range(len(dataset['Y']))])\n",
    "data = dataset['X']\n",
    "n_class = 20\n",
    "\n",
    "# train-test-split\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, label, test_size=0.3, random_state=42)\n",
    "# kf = KFold(10, True, 1)\n",
    "val_acc = []\n",
    "max_index = -1\n",
    "\n",
    "X_train = standardize(X_train)\n",
    "X_test = standardize(X_test)\n",
    "n_sample, n_feature = np.shape(X_train)\n",
    "y = one_hot_encoding(y_train, n_class)\n",
    "\n",
    "# weights = (w_range[1] - w_range[0]) * np.random.random([n_feature, n_node]) + w_range[0]\n",
    "# bias = (b_range[1] - b_range[0]) * np.random.random([1, n_node]) + b_range[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb77e7a6",
   "metadata": {},
   "source": [
    "### 1) Initialization\n",
    "a) Compute $\\mathbf{D}$ where $\\mathbf{D}=\\mathbf{[H,X]}$ <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c0e2ee1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# h = relu(np.dot(X_train, weights) + np.dot(np.ones([n_sample, 1]), bias))\n",
    "# d = np.concatenate([h, X_train], axis=1)\n",
    "# d = np.concatenate([d, np.ones_like(d[:, 0:1])], axis=1) # concat column of 1s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9de2c24",
   "metadata": {},
   "source": [
    "b) Compute $\\mathbf{D}^T\\mathbf{y}, \\mathbf{D}^T\\mathbf{D}$, and its eigenvalues $\\lambda^0_1,\\dots,\\lambda^0_B$ <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "429b135a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.22371836e-11, -2.28070375e-12, -2.09044845e-12, ...,\n",
       "        2.23715050e+05,  3.83469564e+05,  7.23848649e+05])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dT_y = np.dot(d.T, y)\n",
    "# dT_d = np.dot(d.T, d)\n",
    "# eigen_val = np.linalg.eigvalsh(dT_d)\n",
    "# eigen_val"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aec833d",
   "metadata": {},
   "source": [
    "c) Initialize $\\sigma^2$ and $\\gamma$ to default values <br>\n",
    "Evidence approximation (MAP estimation on the posterior of the hyper-parameters):\n",
    "$$p(\\gamma)=\\text{Gamma}(\\gamma \\mid \\alpha_1, \\alpha_2)$$\n",
    "$$p(\\sigma^2)=\\text{Gamma}(\\sigma^{-2} \\mid \\alpha_3, \\alpha_4)$$\n",
    "$$ \\sigma_*^2, \\gamma_*^2 = \\arg\\max \\left\\{ \\int_{\\mathbf{R}^B} p(\\mathbf{y} \\mid \\mathbf{X}, \\mathbf{\\beta}, \\sigma^2)p(\\mathbf{\\beta} \\mid \\gamma) p(\\gamma)p(\\sigma^2)\\,d\\beta \\right\\}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b2e6a470",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evidence approximation\n",
    "basic_model = pm.Model()\n",
    "with basic_model:\n",
    "    weights = pm.Normal('weights', mu=0, tau=1, shape=(n_feature, n_node))\n",
    "    bias = pm.Normal('bias', mu=0, tau=1, shape=(1, n_node))\n",
    "    prec = pm.Gamma('prec', alpha=alpha_1, beta=alpha_2)\n",
    "    var = pm.Gamma('var', alpha=alpha_3, beta=alpha_4)\n",
    "    beta = pm.Normal('beta', mu=0, tau=prec, shape=(n_feature + n_node, n_class))\n",
    "    y_obs = pm.Normal('y_obs', mu=sigmoid_kernel(X_train,beta,weights,bias), tau=var, observed=y)\n",
    "    start = pm.find_MAP()\n",
    "    approx = pm.fit(n_iter, start=start, obj_optimizer=pm.adam())\n",
    "    trace = pm.sample_approx(approx=approx, draws=5000)\n",
    "    pm.traceplot(trace)\n",
    "    pm.summary(trace)\n",
    "    # Mean of 5000 draws\n",
    "    post_pred = pm.sample_ppc(trace, samples=5000, model=basic_model)\n",
    "    y_train_pred = np.mean(post_pred['y_obs'], axis=0)\n",
    "    y_train_pred = np.argmax(y_train_pred, axis=1)\n",
    "    # Evaluate accuracy\n",
    "    train_acc = np.sum(np.equal(y_train_pred, y))/len(y)\n",
    "    X_train.set_value(X_test)\n",
    "    post_pred = pm.sample_ppc(trace,samples=5000,model=basic_model)\n",
    "    y_test_pred = np.mean(post_pred['Y_obs'],axis=0)\n",
    "    y_test_pred = np.argmax(y_test_pred, axis=1)\n",
    "    test_acc = np.sum(np.equal(y_test_pred, y_test))/len(y_test_pred)\n",
    "print(train_acc)\n",
    "print(test_acc)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "53e53f2e48b5b02ad96e830940e12ad58baf9aca82a333a77928015526d4f330"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('pm3env': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
